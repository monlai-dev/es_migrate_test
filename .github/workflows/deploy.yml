name: ES 6.8 GCS Plugin & Verify Flow (No Kibana)

on:
  push:
    branches: [ "main" ]

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      # 1. Build the modified image with GCS Plugin
      - name: Build Docker Image
        run: |
          cat << 'EOF' > Dockerfile
          FROM docker.elastic.co/elasticsearch/elasticsearch:6.8.23
          RUN bin/elasticsearch-plugin install --batch repository-gcs
          EOF
          docker build -t ${{ secrets.DOCKER_USERNAME }}/es68-gcs:latest .

      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Push Image
        run: docker push ${{ secrets.DOCKER_USERNAME }}/es68-gcs:latest

      # 2. SSH to VM to update and verify
      - name: SSH Deploy & Verify
        uses: appleboy/ssh-action@v1.0.3
        with:
          host: ${{ secrets.VM_HOST }}
          username: ${{ secrets.VM_USER }}
          key: ${{ secrets.SSH_PRIVATE_KEY }}
          command_timeout: 30m
          script: |
            set -e  # Exit on any error
            
            # Login to Docker Hub on EC2
            echo "Logging into Docker Hub..."
            echo "${{ secrets.DOCKER_PASSWORD }}" | docker login -u "${{ secrets.DOCKER_USERNAME }}" --password-stdin
            
            # Set the Host Virtual Memory (Required for ES)
            sudo sysctl -w vm.max_map_count=262144
            
            # Save GCS Key from secret to a temp file
            cat << 'GCSKEYEOF' > /tmp/gcs-key.json
            ${{ secrets.GCS_SERVICE_ACCOUNT_JSON }}
            GCSKEYEOF
            
            # Check what's currently running
            echo "=== Current Containers ==="
            docker ps -a
            
            echo "=== Docker Volumes ==="
            docker volume ls
            
            # Find the volume used by the existing ES container (if any)
            EXISTING_VOLUME=$(docker inspect --format='{{range .Mounts}}{{if eq .Destination "/usr/share/elasticsearch/data"}}{{.Name}}{{end}}{{end}}' $(docker ps -aq --filter "ancestor=docker.elastic.co/elasticsearch/elasticsearch:6.8.23") 2>/dev/null || echo "")
            
            if [ -n "$EXISTING_VOLUME" ]; then
              echo "Found existing ES data volume: $EXISTING_VOLUME"
              DATA_VOLUME="$EXISTING_VOLUME"
            else
              echo "No existing volume found, will use: es_data"
              DATA_VOLUME="es_data"
            fi
            
            # 1. Stop BOTH potential containers to free up every drop of RAM
	    echo "Stopping all ES containers to free memory..."
	    docker stop es68 2>/dev/null || true
	    docker stop es-node 2>/dev/null || true
    	    docker rm es-node 2>/dev/null || true

	    # 2. Clean up memory immediately
	    echo "Pruning system to reclaim RAM..."
	    docker system prune -f
            
            # Pull the latest image
            echo "Pulling latest image..."
            docker pull ${{ secrets.DOCKER_USERNAME }}/es68-gcs:latest
            
            # Start NEW container on port 9201 with the SAME data volume
            echo "Starting Elasticsearch container on port 9201 with volume: $DATA_VOLUME"
            docker run -d --name es-node \
              --memory="450m" \
              --memory-swap="450m" \
              -p 9201:9200 \
              -e "discovery.type=single-node" \
              -e "ES_JAVA_OPTS=-Xms192m -Xmx192m" \
              -e "bootstrap.memory_lock=false" \
              -e "cluster.routing.allocation.disk.threshold_enabled=false" \
              -e "http.max_content_length=50mb" \
              -e "indices.memory.index_buffer_size=10%" \
              -e "indices.fielddata.cache.size=15%" \
              -e "indices.queries.cache.size=10%" \
              -e "thread_pool.write.queue_size=200" \
              -e "thread_pool.search.queue_size=500" \
              -v ${DATA_VOLUME}:/usr/share/elasticsearch/data \
              ${{ secrets.DOCKER_USERNAME }}/es68-gcs:latest
            
            # Wait for ES to be ready (with timeout)
            echo "Waiting for ES startup (max 5 minutes)..."
            COUNTER=0
            MAX_TRIES=60
            until curl -s localhost:9201 > /dev/null 2>&1; do
              COUNTER=$((COUNTER+1))
              if [ $COUNTER -gt $MAX_TRIES ]; then
                echo "ERROR: Elasticsearch failed to start within 5 minutes"
                echo "=== Docker logs ==="
                docker logs --tail 100 es-node
                echo "=== Memory usage ==="
                free -h
                echo "=== Docker stats ==="
                docker stats --no-stream es-node || true
                exit 1
              fi
              if [ $((COUNTER % 6)) -eq 0 ]; then
                echo "Attempt $COUNTER/$MAX_TRIES - still waiting..."
              fi
              sleep 5
            done
            echo "✓ Elasticsearch is up on port 9201!"
            
            # Show ES cluster info (to verify it's using the old data)
            echo "=== Elasticsearch Info ==="
            curl -s "localhost:9201" | jq '.'
            
            # Show indices (to confirm data is accessible)
            echo "=== Current Indices ==="
            curl -s "localhost:9201/_cat/indices?v"
            
            # Show current memory usage
            echo "=== Current Memory Usage ==="
            free -h
            docker stats --no-stream es-node
            
            # 3. Inject Credentials into Keystore
            echo "Injecting GCS credentials..."
            docker cp /tmp/gcs-key.json es-node:/usr/share/elasticsearch/config/gcs-key.json
            docker exec es-node bin/elasticsearch-keystore add-file gcs.client.default.credentials_file config/gcs-key.json
            
            # Restart ES (Required in v6.8 for Keystore files to be recognized properly)
            echo "Restarting Elasticsearch..."
            docker restart es-node
            
            echo "Waiting for ES to restart (max 5 minutes)..."
            COUNTER=0
            until curl -s localhost:9201 > /dev/null 2>&1; do
              COUNTER=$((COUNTER+1))
              if [ $COUNTER -gt $MAX_TRIES ]; then
                echo "ERROR: Elasticsearch failed to restart within 5 minutes"
                echo "=== Docker logs ==="
                docker logs --tail 100 es-node
                exit 1
              fi
              if [ $((COUNTER % 6)) -eq 0 ]; then
                echo "Attempt $COUNTER/$MAX_TRIES - still waiting..."
              fi
              sleep 5
            done
            echo "✓ Elasticsearch restarted successfully!"
            
            # 4. VERIFY: Check if GCS plugin is loaded via API
            echo "Verifying GCS Plugin status..."
            curl -s "localhost:9201/_cat/plugins?v"
            if ! curl -s "localhost:9201/_cat/plugins?v" | grep -q repository-gcs; then
              echo "ERROR: Plugin not found!"
              exit 1
            fi
            echo "✓ GCS plugin verified!"
            
            # Check cluster health
            echo "=== Cluster Health ==="
            curl -s "localhost:9201/_cluster/health?pretty"
            
            # 5. START FLOW: Register GCS Repository
            echo "Registering GCS repository..."
            RESPONSE=$(curl -s -X PUT "localhost:9201/_snapshot/gcs_backup" -H 'Content-Type: application/json' -d'{
              "type": "gcs",
              "settings": {
                "bucket": "${{ secrets.GCS_BUCKET_NAME }}",
                "base_path": "backups/v68",
                "compress": true,
                "chunk_size": "10mb"
              }
            }')
            echo "Repository registration response: $RESPONSE"
            
            # Verify repository
            echo "=== Verifying Repository ==="
            curl -s "localhost:9201/_snapshot/gcs_backup?pretty"
            
            # 6. TRIGGER SNAPSHOT
            echo "Starting Snapshot..."
            SNAPSHOT_RESPONSE=$(curl -s -X PUT "localhost:9201/_snapshot/gcs_backup/init_snapshot?wait_for_completion=false")
            echo "Snapshot response: $SNAPSHOT_RESPONSE"
            
            # Check snapshot status
            sleep 3
            echo "=== Snapshot Status ==="
            curl -s "localhost:9201/_snapshot/gcs_backup/init_snapshot?pretty"
            
            # List all snapshots
            echo "=== All Snapshots ==="
            curl -s "localhost:9201/_snapshot/gcs_backup/_all?pretty"
            
            # Cleanup temp file on host
            rm /tmp/gcs-key.json
            
            # Logout from Docker Hub
            docker logout
            
            echo ""
            echo "✓ Deployment complete!"
            echo "=== Final Memory Usage ==="
            free -h
            docker stats --no-stream es-node
            
            echo ""
            echo "=== IMPORTANT ==="
            echo "Your OLD Elasticsearch is on port 9200"
            echo "Your NEW Elasticsearch with GCS backup is on port 9201"
            echo "Both share the SAME data volume: $DATA_VOLUME"